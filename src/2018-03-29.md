---
title: 日報 2018-03-29
---

|||
|:-|:-:|
|9:00~|起床+シャワー+バナナを食す|
|9:30~|スライド作り|
|13:00~|機械学習交流会|
|21:30~|無|

朝起きて、パソコンカタカタしながらバナナをかじる。大変気分がいい。

機械学習交流会のメモ:

### 1. 「Goodfellow氏おすすめGAN論文10本解説」

- Progressive GAN

高画質画像では生成が難しくDiscriminatorが有利になってしまいうまく学習ができなった。
低解像度の画像の生成から始めて、層を増やすことにより段々高画質にしていく。

- Spectral Normalization

Weight Normalizationの新しい手法を提案し、Discriminatorの学習を安定させることが
できた。副次的な結果として多クラスの画像生成が上手く行くようになった。

- Projection Discriminator

従来のConditional GANのDiscriminatorに入力する教師ラベルは単純にconcat等が
行われていたが、教師ラベルも分散表現に落とし込むことにより、AC-GANで
行われるAdversarial LossとClassification Lossのウェイト付けをする必要がなくなる。

- pix2pixHD

すごいよ（動画参照）。

- Are GANs created equal?

どのGANが本当にいいのかの手法が難しくて、チューニング+FIDでどの手法も
あまり差がなかったという結論に。よい評価指標を見つけましょうという話。

- Improved Training of Wasserstein GANs

Lipschitz条件を満たすためこれまではWeight Clipping ($W = \min(\alpha, W$))が
行われていたが勾配のノルムのペナルティを付けた方が学習が安定する。

- StackGAN++

聴き逃した。

- Privacy-preserving

GANで教師データを生成することによって機械学習アルゴリズムにおいて差分プライバシーを
実現。

- GANs with encoders

VAEの変分下界からGANを導出して取り込んだ。

- theory of GAN convergence

GANの学習の収束についての解析。

### 2. 「汎用人工知能」

汎用人工知能のモデル化

### 3. 「基礎からやる確率課程」 

測度論・確率論に関するfree-formな数学に関する講義という感じ。

### 4. 「論文略説: Stochastic Thermodynamics Interpretation of Information Geometry」

ここらへんから追えなくなってきた。最後にDifferentiatial Neural Computerの解説が
あったが、アーキテクチャが大分込み入ってる（!!!微分可能なガベージコレクションも
どきが乗っかってる!!!）ため正直あまり飲み込めなかった。

メモ終わり。

感想としては、発表の多くは相当高いレベルの聴衆を想定していて置いてけぼりに
されてる感じがあった。ただ、普通のLT会と違って観客に合わせてテーマを
簡単に変えたりできない分調整は難しいなとも思うし、なかなか難しい。@latte_zeroによる
話は数日掛けてやるセミナーを数十分でやった感じがあって、数日掛けてやる機会があれば
是非参加して勉強したいなと思った。

交流会が終わってぐたっとしている。資料を作ったものの、残念ながら発表をする時間が
なかった。しかし、@RyoKamoiや先輩に確認したところ既知の内容ではないようで
よかった。自動微分なぜこんなにしられていないんだろうか。当日にスライドを
作り始めたのもあって実装はしていなかったが、もしかしたら実装するかもしれない。

hmatrixによるtype-safeな行列演算とadでスッとNN組もうかなって気持ちになって
調べてみたところ、どうやらadとBLASラッパっぽいライブラリは相性が悪いらしい。
更に色々調べてみたら
[Accelerate用のADライブラリの開発がSummer of Haskellの課題](https://summer.haskell.org/ideas.html#accelerate-automatic-differentiation)
になっているのを見つけた。逆にこれまでなかったのか...と衝撃。

