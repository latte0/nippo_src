---
title: 日報 2018-03-03
---

|||
|:-|:-:|
|11:00~|起床|
|12:30~|昼食|
|13:30~|スクレイピング|
|18:00~|無|
|18:30~|FGO+軽い運動|
|20:30~|風呂掃除|
|21:00~|スクレイピング|
|23:00~|無|

Haskellに熱中してしまい、寝るのが遅くなって起床時間にしわ寄せが来た。

OCamlだとAssociation ListがCoreでサポートされてたりするけど、Haskellでは
マイナーそうなライブラリにあって意外。こういうところにも文化の違いを感じる。

人の欲しいものリストを見るといい本が見つかる。持っている/読んだ本のリストの方が
参考になるっちゃ参考になるけど。

スクレイピング、少し進捗が出たがだいぶ汚い。Tagsoupの設計上多少はしょうがない[^messy]
んだろうけど。

![extractAnimeList: グチャッ...ってしてる](./img/extractAnimeList.png)

[^messy]: ["Do not be robust to design changes, do not even consider the possibility when writing the code."](https://github.com/ndmitchell/tagsoup#readme)

HaskellのShowがデフォルトで日本語をエスケープしちゃう問題はunicode-showで
解決できたっちゃできたんだけどそれとは別途`stack ghci`で上手く日本語の入力・表示
ができない。[^ghci]ghci使えなくていちいち`stack exec scrape`すると著しく
フィードバックループを回す速度が落ちてストレスが溜まる。

[^ghci]: fcitxでmozcに切り替えて入力しようとすると???って入力されて、日本語が
入ったStringを出力しようとすると同様に???って出る。

本アドベントカレンダーが4日目で止まっていることを思い出した。

![extractAnimeList: 地獄](./img/extractAnimeList2.png)

限界がやってきた。スクレイピングライブラリの限界というよりWikipediaの記述の
仕方がパースしにくいことが原因。載っている声優の数はだいぶ少ないが、
<http://lain.gr.jp/voicedb>をスクレイプすることを検討している。

YouTubeをiframe埋め込みした途端ページが極端に重くなることに気づいた。対策として
まず思い浮かぶのは<http://motherfuckingwebsite.com/>で有名な[txti](http://txti.es/)
の<http://txti.es/images>みたいなやり方だがPandocフィルタでも使わないと
実現できなさそうだ。日報のためにそこまでするか？という話がある。

.lainのスクレイピングがめちゃくちゃすんなりいっている。

```haskell
extractAnimeList =
    fromJust . scrape g
  where
    g :: Scraper T.Text [Anime]
    g = chroots ("ul" @: [ hasClass "cast-list" ] // "li") getAnime
    getAnime :: Scraper T.Text Anime
    getAnime = do
      title <- text $ "h3" // "a"
      href <- attr "href" $ "h3" // "a"
      roleName <- text $ "div" @: [ hasClass "role" ]
      return $ Anime title href roleName
```

今までの苦労はなんだったんだ。

とりあえず声優一覧はとれた。Aesonで適宜JSONからserialize/deserializeして
使おう。

```haskell
{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE DeriveGeneric #-}
module Main where
import Data.Aeson
import Data.Aeson.Text
import Data.Maybe
import Control.Lens
import Control.Monad
import Text.HTML.TagSoup
import Text.HTML.Scalpel
import qualified GHC.IO.Encoding as E
import GHC.Generics
import Text.Show.Unicode
import qualified Text.StringLike as TS
import qualified Network.Wreq as W
import qualified Data.Text as T
import qualified Data.Text.Lazy as TL
import qualified Data.Text.IO as T
import qualified Data.Text.Encoding as E
import qualified Data.ByteString.Lazy as B

import Debug.Trace
 
openURL :: String -> IO T.Text
openURL loc =
    E.decodeUtf8 . B.toStrict . (^. W.responseBody) <$> W.get loc 

data Anime = Anime
  { title :: T.Text
  , animeHref :: T.Text
  , roleName :: T.Text
  } deriving (Generic, Show)

instance ToJSON Anime
instance FromJSON Anime

data Actor = Actor
  { name :: T.Text
  , actorHref :: T.Text
  , isMale :: Bool
  } deriving (Generic, Show)

instance ToJSON Actor
instance FromJSON Actor

actorListURL :: Int -> String
actorListURL n = "http://lain.gr.jp/voicedb/profile/list/cid/" ++ show n

extractAnimeListFromURL :: String -> IO [Anime]
extractAnimeListFromURL =
  fmap (extractAnimeList . parseTags) . openURL

extractAnimeList :: [Tag T.Text] -> [Anime]
extractAnimeList =
    fromJust . scrape g
  where
    g :: Scraper T.Text [Anime]
    g = chroots ("ul" @: [ hasClass "cast-list" ] // "li") getAnime
    getAnime :: Scraper T.Text Anime
    getAnime = do
      _title <- text $ "h3" // "a"
      _href <- attr "href" $ "h3" // "a"
      _roleName <- text $ "div" @: [ hasClass "role" ]
      return $ Anime _title _href _roleName

extractActorListFromURL :: String -> IO [Actor]
extractActorListFromURL =
  fmap (extractActorList . parseTags) . openURL

extractActorList :: [Tag T.Text] -> [Actor]
extractActorList =
    fromJust . scrape g
  where
    g :: Scraper T.Text [Actor]
    g = chroots ("ul" @: [ hasClass "listItems" ] // "li") getActor
    getActor :: Scraper T.Text Actor
    getActor = do
      _name <- text $ "a"
      _href <- attr "href" $ "a"
      _isMale <- fmap (== "/images/voicedb/2.png") . attr "src" $ "img"
      return $ Actor _name _href _isMale

main :: IO ()
main = do 
    E.setLocaleEncoding E.utf8
    result <- concat <$> mapM (extractActorListFromURL . actorListURL) [1..10]
    T.putStrLn . TL.toStrict $ encodeToLazyText result
```

コメントアウトされた煩雑なコードを除くと割とスッキリしたコードになる。

```bash
$ stack exec scrape > actorlist.json
$ jq 'map(.name)' actorlist.json | wc -l
4126
```

[mt-caret/actorlist.json](https://gist.github.com/mt-caret/0c0981b71643f968af15ccb0418a5576)

iframeを入れるの真摯でない気がしてきた。リンクにしよう。

cmusなりMopidy+ncmpcppなりでメインマシン上で持っているCDの曲を聞く方法を
確立したいけどメタデータの入力が面倒くさい。クリエータが気軽に自分が作った
曲のメタデータを登録できるオープンなデータベース、絶対あるはずだし[^sdb]同人系の
曲でメタデータが利用できないのはただ浸透してないだけなのか、別な理由があるのか
気になる。

[^sdb]: Gracenoteってオープンなんだっけ？

TagsoupとScalpelの扱い方つかめてきた。基本的にScalpelを使って、neighborベースの
抽出とかが必要になったタイミングで`innerHTML anySelector`で生HTMLを抽出した後
Tagsoupの`parseTags`で`[Tag str]`に変換する感じ。

Brainfuck系言語を簡単に作れるサービス、作りたいなって思ってたら:
<https://twitter.com/uhyo_/status/969908168087150593>

### 今日のBGM

- [20170701](https://www.youtube.com/watch?v=irko_QJ5sXk)
- [NERD & BREKCORE MIX ](https://soundcloud.com/tobato/nerd-brekcore-mix)

## 明日やるべきこと

- メール
- スクレイピングが上手くいったら#haskell-beginnerのshaprに報告すること
- 3DMM
- 画像処理調べもの
- Rust
- 確定申告・精算
- Econometrics

